<!DOCTYPE HTML>
<html>
<head>
    <title>Module 1: Data Visualization - Unit 8</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <style>
        p {
            text-align: left; /* Aligns all paragraph text to the left */
        }
    </style>
</head>
<body class="is-preload">

    <!-- Navigation back to module -->
    <nav id="nav">
        <ul class="container">
            <li><a href="mod_6_main.html">Back to Module 6: Research Methods Learning</a></li>
        </ul>
    </nav>

    <!-- Unit Content -->
<article id="unit-content" class="wrapper style1">
    <div class="container">
        <section id="reflection">

<h2>Peer Review of Research Methods in AI and Academic Writing<br>
Reflective Piece (Using Driscoll’s Model)</h2>

<a href="docs\M6U3PeerReviewActivity.pdf"
   class="button large scrolly" target="_blank">
   View Peer Review Submission
</a>
<br><br>

<h3>What?</h3>
<p>
    This Unit 3 seminar activity focused on evaluating existing literature through peer review, with an emphasis on research design and methodology. For this task, I reviewed two papers addressing AI use in academic writing, one by Khalifa and Albadawy (2024), which used a systematic review approach, and another by Mondal, Mondal and Jana (2025), which adopted a perspective-based methodology. In my submission, I examined how clearly each paper defined its purpose, whether the chosen research method was suitable for that purpose, and how effectively data and evidence were used to support the authors’ claims. I also identified methodological limitations and suggested concrete ways each paper could be improved. This activity directly required me to move beyond summarising literature and instead engage with how research is constructed and justified.
</p>

<h3>So what?</h3>
<p>
    Reflecting on this exercise made me realise that I had previously been too generous when reading academic papers, especially in fast-moving areas like AI. I often accepted conclusions at face value if they sounded reasonable or aligned with my own experiences. This task forced me to slow down and ask more uncomfortable questions, particularly about whether the research method actually supported the strength of the claims being made. For example, while reviewing the systematic review paper, I became more aware of how easily a lack of critical appraisal can weaken what initially appears to be a robust methodology.
</p>
<p>
    Comparing the two papers also helped me understand that not all weaknesses are equal. The perspective paper was not flawed because it lacked empirical data, but because its conclusions occasionally extended further than the evidence justified. This distinction was important for me. It showed that the issue is not whether a paper uses qualitative or quantitative methods, but whether the authors are honest about the limits of those methods. This shifted how I now judge academic quality, placing less emphasis on labels like “systematic review” and more on transparency, proportionality, and methodological discipline.
</p>

<h3>Now what?</h3>
<p>
    Going forward, this reflection will directly shape how I approach both reading and writing academic work. When selecting sources, I am now more attentive to how evidence is gathered, how competing viewpoints are handled, and where authors may be overstating certainty. This is particularly relevant for AI-related research, where enthusiasm for innovation often runs ahead of robust evidence. I am also more confident in critiquing literature constructively, rather than treating published work as inherently authoritative.
</p>
<p>
    In my own academic writing, I will apply these lessons by being clearer about the limits of my methods and more cautious in how I frame conclusions. Professionally, this skill is equally important. In data science and computing roles, research is often used to justify decisions that affect policy, assessment, or people’s careers. Being able to identify weak methodology or overconfident claims is not just an academic exercise, but a necessary part of responsible professional judgment. This activity reinforced that critical evaluation is a skill that must be practised deliberately, not assumed to develop automatically.
</p>

<h3>References</h3>
<ul>
    <li>
        Khalifa, M. and Albadawy, M. (2024) ‘Using artificial intelligence in academic writing and research: An essential productivity tool’,
        <em>Computer Methods and Programs in Biomedicine Update</em>, 5, 100145.
    </li>
    <li>
        Mondal, H., Mondal, S. and Jana, S. (2025) ‘The Artificial Intelligence Dilemma in Academic Writing: Balancing Efficiency and Integrity’,
        <em>Indian Journal of Cardiovascular Disease in Women</em>, 10(3), pp. 225–230.
    </li>
    <li>
        Driscoll, J. (2007) <em>Practising Clinical Supervision: A Reflective Approach for Healthcare Professionals</em>. 2nd ed. Edinburgh: Elsevier.
    </li>
</ul>

        </section>
    </div>
</article>



            
            <!-- Buttons to open documents -->
            <footer>
                
            </footer>
        </div>
    </article>
</body>
</html>
