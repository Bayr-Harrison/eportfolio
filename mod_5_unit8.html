<!DOCTYPE HTML>
<html>
<head>
    <title>Module 1: Data Visualization - Unit 8</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <style>
        p {
            text-align: left; /* Aligns all paragraph text to the left */
        }
    </style>
</head>
<body class="is-preload">

    <!-- Navigation back to module -->
<nav id="nav">
    <ul class="container">
        <li><a href="mod_5_main.html">Back to Module 5: Machine Learning</a></li>
    </ul>
</nav>

<!-- Unit Content -->
<article id="unit-content" class="wrapper style1">
    <div class="container">
        <section id="reflection">
            <h2>Evaluating the Role of AI in Aviation Assessment (Driscoll’s Reflective Model)</h2>

            <!-- Artefact link(s) -->
            <div class="button-row">
                <a href="docs/AI As a Tool Not a Substitute.pdf" class="button large scrolly" target="_blank">
                    View Initial Post (PDF)
                </a>
            </div>
            <br>

            <h3>What?</h3>
            <p>
                In this unit’s discussion, I reflected on my direct experience with the introduction of AI tools, particularly large language models, into the aviation maintenance exam department. These tools were intended to support the rapid generation of large numbers of summative exam questions. Initially, the output appeared promising, producing fluent and well-structured text. However, once deployed, the generated questions revealed significant issues, including references to non-existent figures, fabricated content, and inconsistent difficulty levels. 
            </p>
            <p>
                This created unexpected challenges for both Subject Matter Experts (SMEs) and students. For non-native English speakers, overly complex or poorly structured language amplified confusion. The workload, rather than being reduced, shifted toward reviewing, correcting, and rewriting a large volume of mediocre questions. What began as an efficiency solution ultimately resulted in more time spent ensuring quality and compliance with aviation standards.
            </p>

            <h3>So What?</h3>
            <p>
                This experience demonstrated the gap between the perceived capabilities of AI and its actual reliability in high-stakes educational contexts. As Hutson (2021) points out, large language models are “a mouth without a brain,” capable of generating plausible text without true understanding. In practice, this lack of comprehension manifested in the production of flawed exam content that required human intervention to correct.
            </p>
            <p>
                Bender et al. (2021) argue that language models reproduce patterns without understanding, which aligns with my observation of the AI’s ability to mimic question formats without adhering to the logic or referencing accuracy expected in aviation assessments. Carlini et al. (2021) also warn of the risks associated with data leakage and unreliable outputs. In this case, the issue was not data security but the reliability and validity of the generated content. 
            </p>
            <p>
                Reflecting on this, I realised that the implementation failed not because the technology itself was inherently flawed, but because it was treated as a replacement rather than a tool to assist SMEs. The absence of robust review workflows and clear usage boundaries amplified the consequences of AI’s limitations. This has reinforced my understanding that in regulated fields like aviation, quality assurance and domain expertise remain irreplaceable.
            </p>

            <h3>Now What?</h3>
            <p>
                Moving forward, I intend to advocate for a structured integration of AI tools in exam development rather than outright replacement. This involves defining clear boundaries for where AI can add value, such as generating draft question stems or providing linguistic variations, while ensuring SMEs remain responsible for accuracy, referencing, and alignment with learning outcomes.
            </p>
            <p>
                I also plan to develop a quality assurance workflow that integrates AI outputs into existing SME review processes, rather than bypassing them. This will include checklists for content validation and language calibration, especially for non-native English learners. On a personal level, this reflection has strengthened my resolve to approach technological tools critically and strategically, recognising both their potential and their limitations.
            </p>
            <p>
                In future implementations, I will ensure that AI tools are positioned as assistants rather than substitutes, especially in high-stakes contexts where errors can compromise educational standards and student outcomes.
            </p>

            <h3>References</h3>
            <ul>
                <li>Bender, E.M., Gebru, T., McMillan-Major, A. and Shmitchell, S., 2021. On the dangers of stochastic parrots: can language models be too big? <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, pp. 610–623.</li>
                <li>Carlini, N. et al., 2021. Extracting Training Data from Large Language Models. <em>USENIX Security Symposium</em>, pp. 2633–2650.</li>
                <li>Hutson, M., 2021. Robo-writers: the rise and risks of language-generating AI. <em>Nature</em>, 591(7848), pp. 22–25.</li>
                <li>Driscoll, J., 2007. <em>Practising Clinical Supervision: A Reflective Approach for Healthcare Professionals</em>. 2nd ed. Edinburgh: Elsevier.</li>
            </ul>
        </section>
    </div>
</article>



            
            <!-- Buttons to open documents -->
            <footer>
                
            </footer>
        </div>
    </article>
</body>
</html>
